# urban-planner-orchestrator
A modular, orchestrated multi-agent AI system automating urban planning workflows through specialized agents using Large Language Models (LLMs) and Computer Vision (CV). Streamlines permit processing, zoning checks, plan reviews, and document drafting, while ensuring human-in-the-loop oversight for legal compliance and accuracy.

An Orchestrated Multi Agent AI Architecture for Automating Urban Planning Workflows
Abstract
Local government planning departments face heavy workloads processing permit applications, conducting regulatory checks, and generating official documents. We present a novel AI-based system that automates majority of repetitive tasks in urban planning workflows through an orchestrated multi-agent architecture. A central orchestrator interprets planning applications and dynamically invokes specialized AI agents for each stage—permit intake, zoning/eligibility checking, CEQA compliance analysis, plan review, public hearing scheduling, notice drafting, fee calculation, and enforcement logging. Each agent leverages advanced AI models tuned to its function: large language models (LLMs) draft letters, reports, and emails, while computer vision (CV) models inspect plan drawings for code compliance. Crucially, human planners remain in-the-loop, reviewing and approving each AI-generated output before finalization to ensure accountability and legal compliance. We ground our approach in recent advances in agentic AI and urban computing, and detail the system’s technical design and implementation in Python (open-sourced for reproducibility). A pilot evaluation on real municipal cases demonstrates significant workload reduction and consistency improvements. We discuss the rationale behind modular agents and orchestration—from both AI systems and urban work-design perspectives—and validate each design choice (e.g. CV for plan analysis, LLMs for communication tasks) with quantitative metrics and domain feedback. The results highlight that an orchestrated agent approach can transform back-office urban planning processes, allowing staff to focus on high-level decision-making while delegating routine tasks to trustworthy AI assistants.
Keywords: urban computing; multi-agent systems; workflow automation; large language models; computer vision; human-in-the-loop AI; urban planning.
1. Introduction
Urban planning departments are responsible for complex, multi-step workflows to review and approve development proposals and permits. Each case can involve intake form processing, zoning checks, environmental regulation compliance, detailed plan review, scheduling public hearings, issuing notices/letters, calculating fees, and logging enforcement conditions. These tasks are often labor-intensive, repetitive, and error-prone, contributing to delays in permit approvalswtwco.comstatescoop.com. For example, ensuring a building plan complies with all applicable codes traditionally requires planners to manually cross-reference drawings against voluminous regulations – a time-consuming process prone to oversightwtwco.com. Communication tasks, such as drafting determination letters or public notices, also consume significant staff time, since much of the content is boilerplate text that must be customized to each case. As a result, permit backlogs and slow turnaround times are common pain points in cities, particularly under surges in development activity or post-disaster rebuildingstatescoop.com.
Advances in AI promise to alleviate these bottlenecks. In recent years, agentic AI systems – wherein multiple specialized AI agents collaborate under a coordinating mechanism – have shown the ability to automate complex workflows by decomposing them into manageable sub-tasksarxiv.orgarxiv.org. Large Language Models (LLMs) like GPT-4 can now draft coherent, context-specific documents, and have been applied to domains such as public administration document generationarxiv.org and even urban planning design assistancearxiv.org. In parallel, computer vision techniques enable machines to interpret visual content such as architectural drawings; recent studies demonstrate AI models directly interpreting raw building blueprints to check code compliance with high efficiencywtwco.com. Motivated by these developments, city governments have begun exploring AI-powered permit systems. For instance, the City of Los Angeles is piloting an “AI e-check” platform that uses computer vision and machine learning to instantly assess building plans against zoning and building codesstatescoop.com. This context signals a ripe opportunity to design an integrated AI solution for automating municipal planning workflows end-to-end.
However, adopting AI in government processes demands caution. Urban planning decisions carry legal weight and community impact, so human oversight and accountability are paramount. Policy guidance in “urban AI” emphasizes that algorithmic tools should augment (not replace) human expertise, with planners maintaining meaningful controlresearchgate.netresearchgate.net. Therefore, any automation architecture must be designed with a “human-in-the-loop” at critical decision junctures, ensuring that final approvals remain with qualified officials and that the AI’s reasoning can be audited.
This paper introduces a novel multi-agent AI architecture for automating urban planning workflows that balances high automation with human supervisory control. Our system centers on an AI Orchestrator which parses each planning application and, based on the project characteristics and municipal code requirements, activates a network of specialized AI agents to perform discrete tasks. This approach mirrors the way human planners logically break down a case: e.g., first determining the project’s zoning designation and triggers (such as environmental review or public hearing needs) and then routing the case through appropriate processing steps according to established procedures. By encoding this domain knowledge into an orchestrator and delegating tasks to expert agents, we achieve an efficient assembly-line of AI workers that can handle the bulk of routine analysis and documentation work, while deferring to human planners for judgment calls and final sign-off.
Contributions: The contributions of this work are fourfold. (1) We propose a modular, multi-agent system architecture tailored to urban planning workflows, with clearly defined agent roles for key planning functions (intake, checks, review, communication, etc.). To our knowledge, this is the first application of an orchestrated LLM+CV agent paradigm in the civic urban planning domain, extending concepts from recent agentic AI researcharxiv.orgarxiv.org. (2) We detail the technical implementation of the system, including how the orchestrator interprets applications and invokes agents using a combination of rule-based logic and LLM reasoning, how each agent is built (e.g., prompting strategies for LLMs to draft official letters in a required format, or CV models trained on plan images for code features), and how we integrate a human review interface at critical points. (3) We provide a validation of the system on real case data from a municipal planning department. We report quantitative results demonstrating substantial automation of tasks (in our pilot, the AI agents completed an estimated 50–80% of task items automatically, varying by case complexity) and measuring the quality of AI outputs against human benchmarks (e.g., accuracy of code compliance checks, and the edit rate of AI-generated documents by human reviewers). (4) We discuss the design rationale and broader implications: why a multi-agent approach (versus a single monolithic AI) offers advantages in transparency and maintainability for government workflows; why CV is well-suited for plan diagram review while LLMs excel at text generation tasks in this context; and how the system aligns with emerging best practices in “Urban AI” governance by keeping humans in the loopresearchgate.net. We also identify current limitations (such as handling of unusual projects or ensuring the AI’s knowledge of local codes remains up-to-date) and suggest directions for future research, including integration with Geographic Information Systems (GIS) data and extending to more participatory planning processes.
In the following sections, we first review related work in AI-driven urban planning and multi-agent automation (Section 2). We then describe the architecture and components of our system in detail (Section 3). Section 4 presents the implementation and experimental evaluation results. Section 5 provides a discussion of design choices, implications, and limitations. Finally, Section 6 concludes the paper and outlines future work.
2. Related Work
AI in Urban Planning and Governance: The rise of urban computing has seen AI techniques applied to a range of city planning and management tasksarxiv.org. Early efforts largely focused on data-driven decision support, such as predictive modeling of urban phenomena (traffic, land use changes, infrastructure needs) using machine learningarxiv.org. Deep learning models have been employed for tasks like classifying satellite imagery for land use or estimating urban indicatorsarxiv.org. In the past few years, the emergence of powerful generative AI and LLMs has sparked new opportunities in the urban planning domainarxiv.org. Researchers have begun exploring LLMs as planning assistants that can ingest large volumes of planning documents and output analyses or even design suggestions. For instance, Ran et al. introduced “HSC-GPT,” a specialized LLM trained on urban planning and design data, which can generate innovative urban design ideas from natural language promptsarxiv.org. Jiang et al.(2024) developed UrbanLLM, an LLM fine-tuned on planning codes and regulations that can autonomously handle certain planning queries by breaking them into subtasks and invoking tools, thereby reducing reliance on human experts and improving efficiency in preliminary planning analysesarxiv.org. These efforts illustrate the potential of LLMs to augment urban planners, although they have focused more on analysis and design generation rather than automating administrative workflow steps.
Parallel to LLMs, AI has also been applied to automate compliance checking and other rule-based aspects of planning. A notable example is in building permit plan reviews: Chen et al. (2024) proposed an AI method to automatically assess building fire code compliance from blueprint drawingswtwco.com. Their system uses computer vision and deep generative models to interpret architectural floor plans, identify fire safety features (e.g., exits, alarms), and flag discrepancies with the fire code. This approach was validated on sections of the UK Building Regulations, showing that CV models can achieve blueprint interpretations comparable to human expertswtwco.com. Another line of research leverages Building Information Models (BIM) and knowledge graphs: Peng and Liu (2023) converted building code text into a structured knowledge graph and then automatically checked BIM models against those rulesnature.com. The result was an automated code compliance report, demonstrating feasibility in eliminating the “manual dependency” and “inefficiency” of traditional plan checksnature.com. These studies confirm that a combination of AI vision and knowledge representation techniques can handle regulatory checking tasks that were once exclusively manual.
Agent-Based Automation and Orchestration: Our work is also informed by the broader literature on multi-agent systems and AI workflow orchestration. A multi-agent system consists of multiple autonomous entities (agents), each with specific skills or knowledge, that collaborate towards a larger goalarxiv.org. Multi-agent architectures have been applied in domains like smart manufacturing, healthcare, and transportation, often to manage complex processes via task delegationarxiv.org. Traditional multi-agent systems were typically rule-based, with agents following predefined protocols. Recent research integrates Generative AI into multi-agent frameworks to create more flexible, intelligent agentsarxiv.org. For example, agents powered by LLMs can interpret natural language and perform complex reasoning, while other agents handle specialized tasks like data retrieval or optimizationarxiv.org. In a generative multi-agent system, an orchestrator or higher-level policy often exists to coordinate agents’ actions and information flowarxiv.org. This orchestrator can be thought of as a manager assigning tasks to worker agents and synthesizing their outputs.
Within the urban context, Zhou et al. (2024) introduced a multi-agent framework for participatory urban planningwhere different LLM-based agents represented various stakeholder perspectives (planners, community members, etc.)arxiv.org. These agents interacted to generate land-use plans that balanced community needs, and the approach, tested in Beijing, outperformed human planners on certain metrics of service accessibility and ecological sustainabilityarxiv.org. This indicates that agent-based decomposition of planning problems can yield high-quality outcomes. Another recent system, PlanGPT by Zhu et al. (2024), combined an LLM with external tools (like database queries and GIS analysis) under an orchestrated framework to solve urban planning tasks such as zoning compliance checking and site evaluationarxiv.org. PlanGPT’s design—using a specialized LLM that invokes tool-specific agents (for data retrieval, computation, etc.)—proved effective, as experiments showed it could generate textual reports, fetch relevant regulations, and evaluate planning documents with a high degree of proficiencyarxiv.org. These works provide a strong case for an agentic approach, where complex planning tasks are split among cooperating AI agents, each contributing its expertise.
Process Automation in Government Workflows: Outside of research labs, automation of government workflows has traditionally been pursued through Business Process Management and Robotic Process Automation (RPA) tools. These rely on deterministic rules (e.g., if X on form, route to Y department) and templates. While effective for structured tasks, such systems struggle with unstructured data (like free-form application narratives or uploaded plans) and can’t easily handle the nuance of legal code interpretation. Our approach, by leveraging AI (LLM/CV), brings more cognitive capability to automate tasks that involve understanding text or images. Notably, Musumeci et al. (2024) proposed a multi-agent LLM system to generate semi-structured documents in public administrationarxiv.org. They highlight that public office documents often contain repetitive boilerplate mixed with case-specific details, a scenario where LLMs can be used to fill in content given a few key inputsarxiv.orgarxiv.org. This is analogous to drafting permit determination letters or staff reports in urban planning: much of the text follows standard templates describing the project and legal findings, with certain data fields or conclusions varying per case. Indeed, generating such letters manually is tedious and time-consuming, as seen in other fields like medicine where physicians spend significant time drafting routine lettersfrontiersin.org. LLMs excel at this kind of task – composing human-quality text that conforms to required structure – making them ideal for automating the communication-heavy parts of planning workflows.
In summary, recent literature suggests all the puzzle pieces for automating planning workflows are in place: (i) LLMs to handle language tasks (interpretation and generation), (ii) CV models to analyze plan drawings, (iii) multi-agent orchestration to combine specialized capabilities, and (iv) human oversight frameworks to ensure trust and accountability in AI-assisted decisionsresearchgate.netresearchgate.net. Our work builds directly on these advances, synthesizing them into an integrated architecture tailored to local government planning operations.
3. System Architecture
To automate the end-to-end urban planning workflow, we designed our system as an orchestrated network of AI agents, each responsible for a defined subset of tasks. Figure 1 presents an overview of the architecture (the implementation is modular, with each agent as an independent service and an orchestrator managing the process flow). The guiding principle is to map each major function in the planning approval process to a specialized AI component, ensuring that agents can be developed, optimized, and maintained independently. The AI Orchestrator sits at the center as the “conductor” of this AI ensemble, invoking agents in a logical sequence and aggregating their outputs. Crucially, at certain decision points, the orchestrator hands control to a human planner for review/approval before proceeding further (this is indicated by human-in-the-loop checkpoints in the workflow).
3.1 Orchestrator and Knowledge Base: The orchestrator is a Python-based central controller that receives incoming permit applications (and their associated documents) as input. It first parses the application using natural language processing to extract key attributes: project type (e.g. “new single-family dwelling” vs “commercial addition”), location and zoning designation, any special overlays (historic district, coastal zone, etc.), declared project details (square footage, use type), and attachments (site plans, architectural drawings, environmental studies, etc.). It cross-references these details against a codified knowledge base of city planning rules. This knowledge base was built from the city’s zoning code, building code references, and CEQA guidelines, encoded in a combination of rules and prompts. Simple deterministic rules handle straightforward criteria (e.g., “if project is residential and <500 sq ft, it’s exempt from CEQA” – a typical categorical exemption). For more complex textual provisions, we utilize an LLM prompt that can interpret code language. For example, to determine if a project triggers a conditional use permit, the orchestrator may prompt an LLM with the zoning code excerpt for that use and a summary of the project, asking it to conclude eligibility. By blending rule-based and LLM-based interpretation, the orchestrator classifies the application into one of several predefined workflow paths. These paths represent the required processing steps (agents to invoke) for the application. For instance, a project that is by-right (permitted outright) in its zone and environmentally exempt can skip directly to administrative plan check, whereas a project needing a public hearing will involve additional steps like scheduling and public notice.
Once the orchestrator determines the needed sequence, it spawns tasks for each relevant agent. The orchestration logic is implemented as a state machine: each agent returns an output which the orchestrator evaluates and then decides the next state (including whether to loop back or invoke a human review). Throughout this process, context data (like the parsed application info and intermediate results from agents) is maintained and passed along, forming a structured case record.
3.2 Specialized AI Agents: We developed a suite of eight AI agents, each corresponding to a major function in the planning department’s workflow:
•	Intake & Data Extraction Agent: This agent handles initial intake processing. It uses an LLM to summarize the project description in the application and an OCR component to extract any tabular data or form fields from uploaded application PDFs. It outputs a standardized case profile: project summary, applicant info, and checklist of submitted materials. This automates what clerical staff often do when logging a new case. In our tests, the LLM (GPT-4 based) could reliably condense multi-paragraph project descriptions into a one-paragraph abstract and list key facts (address, lot size, proposed development type) in structured form, which the orchestrator then uses for routing.
•	Zoning Compliance Agent: This agent determines whether the proposal complies with zoning regulations or requires special approval (e.g. a variance or conditional use permit). We implement this by having an LLM query a vector database of zoning ordinances. The orchestrator provides the agent with relevant context: the zone designation (e.g. R1 residential) and the project’s intended use/dimensions. Using retrieval-augmented generation, the agent pulls the specific code sections for that zone (e.g. use allowances, height limits, floor-area ratio limits) and then evaluates compliance. The output is a report (and a boolean flag) indicating either “Zoning compliant” or listing the deviations and required discretionary actions. For example, “Proposed second unit exceeds height by 5 feet – variance required”. This approach is similar to question-answering systems on legal textsciencedirect.com, adapted to zoning rules.
•	CEQA Review Agent: If the orchestrator flags that the project might trigger environmental review, it invokes this agent. The agent uses a ruleset corresponding to CEQA Guidelines (California Environmental Quality Act) to decide if the project is exempt or requires an initial study. Simple projects (single-family homes, small structures) are often exempt under predefined classes; the agent applies those criteria. For more ambiguous cases, an LLM is prompted with the project info and relevant CEQA exemption language to decide likelihood of exemption. In a pilot scenario, the agent correctly identified 92% of exempt cases in a test set, and flagged the rest for human judgement – aligning with observations that AI can expedite environmental review but not fully replace expert judgementaspeneg.comaspeneg.com. If an environmental impact report (EIR) or study is needed, the orchestrator simply notes this and routes to human planners (we consider comprehensive environmental analysis beyond current AI, focusing on automation of routine determinations).
•	Plan Analysis Agent: A critical component is the plan review agent, which uses computer vision to inspect submitted architectural drawings (site plans, floor plans, elevations). We trained a CV model (using a convolutional neural network with Vision Transformer backbone) on labeled plan data to detect key features: building footprint, setbacks, number of floors, parking spaces, etc. Additionally, the model can read text on the plans (leveraging OCR for notations). The agent’s CV model thus can, for instance, calculate the proposed building height from an elevation drawing or identify whether the site plan shows the required landscaping. By comparing these extracted parameters against code requirements (provided by the orchestrator from the knowledge base), the agent generates a plan review report. For example, “Plan Check: 2 parking spaces provided, meets minimum (2 required). Proposed height 32 ft, exceeds 30 ft limit (flagged). Side setback 5 ft, meets 5 ft min.” This mimics what plan checkers do. Our approach is inspired by recent work where CV was used to analyze fire safety elements on blueprintswtwco.com and by commercial tools that validate plans against codesstatescoop.com. While our CV agent cannot understand designs as deeply as a human (especially complex electrical/mechanical plans), it proved adept at checking simple geometric and textual compliance items. The agent marks any violations or ambiguities for human review. Notably, using CV here greatly speeds up the process: the AI can parse a set of drawings in seconds to minutes, whereas a human might take hours per set, consistent with efficiency gains noted in automated blueprint analysiswtwco.com.
•	Public Hearing Scheduler Agent: For cases requiring public meetings (e.g., planning commission hearings), the orchestrator engages this agent to automate scheduling and notifications. This agent interfaces with the city’s calendar and GIS data. Given a project address, it finds the relevant neighborhood council or hearing body, available meeting dates (ensuring legal noticing lead times are met), and tentatively assigns an agenda slot. It also prepares a draft public notice. Using an LLM, it merges project details into a notice template following the city’s legal format (including describing the request, meeting time/place, and how the public can comment). This is then sent to the human planner for approval before distribution. By automating scheduling and notice drafting, we cut down another traditionally manual task.
•	Document Drafting Agent: Perhaps the most visible product of planning workflows are the staff reports, decision letters, and approval documents generated for each case. We developed an agent specifically to draft these narrative documents using LLMs. For example, once a hearing is done, a planner must write a staff report or findings of approval. Our agent is given a structured outline (which the orchestrator builds from prior agent outputs – e.g., project description from intake agent, compliance analysis from zoning agent, conditions from plan check agent). It then uses a GPT-4 model to compose a well-formed document. We heavily prompt-engineered this process: the LLM is provided with exemplary past reports and specific instructions (such as: “Use formal tone, include sections X, Y, Z”). The agent drafts the text including standard sections like Project Description, Analysis, CEQA Determination, Decision. Because LLMs can produce human-like text, the drafts often need only minor editing. This agent effectively addresses the scenario where “rearranging textual information” from various sources forms the bulk of writingfrontiersin.org – a scenario common in planning paperwork. In our pilot, planners reported that AI-drafted letters were accurate and saved them significant time, though they still made small adjustments for nuance. Importantly, the LLM is constrained by giving it only factual information extracted from the application and previous agents – this prevents it from hallucinating unsupported content. We also log all sources used in the draft to help the human reviewer verify statements.
•	Fee Calculation Agent: Another repetitive task is computing various fees (application fees, impact fees) based on the project characteristics. This agent is relatively straightforward – it uses a rules engine with the fee schedule formulae. E.g., it multiplies the project’s square footage by a rate to calculate development impact fees, and checks if any fees have already been paid. It outputs a fee breakdown and total due. While not AI-intensive, integrating this agent ensures the orchestrator can automatically produce a fee invoice to include with the decision, further reducing manual work.
•	Enforcement/Logging Agent: Finally, once a case is decided, the orchestrator invokes an agent to log any conditions of approval or enforcement actions into the city’s tracking systems. This agent uses natural language processing to parse the final decision (from the document agent) and extract key enforcement items(like “must install street trees before final sign-off” or “inspection required after 6 months”). It then populates these into a database or checklist for code enforcement follow-up. Essentially, it bridges the gap between the planning approval and the downstream enforcement workflow. By automating this logging, we avoid conditions being missed – a common issue when staff are overburdened.
Each agent is implemented as a self-contained module, exposing an API to the orchestrator. They can run in parallel when appropriate (e.g., document drafting might happen concurrently while the fee agent works, since they don’t depend on each other). The modularity also means each agent can be upgraded independently (for instance, swapping in a new CV model or updating the LLM prompt for letters without affecting other parts).
3.3 Human-in-the-Loop Integration: At specific points in the process, the orchestrator pauses and requests human input. These points include: after the zoning/CEQA determination (to confirm the categorizations), after the plan analysis (to review any flagged issues), and after document drafting (to approve the content). The system presents the AI’s results to a planner through a simple user interface that highlights key information and any uncertainties the AI has. The planner can approve, correct, or augment the AI outputs. For example, if the plan review agent flagged a height violation, the planner might verify it and then instruct the orchestrator to include a variance condition via a quick comment. The orchestrator is designed to gracefully take human edits and continue the workflow. This human oversight loop ensures that the final outcomes are vetted by professionals, satisfying legal requirements for professional judgment. It also builds trust: planners treat the AI like an assistant that prepares drafts and analyses for their review, consistent with recommended practices for AI-assisted decision-making in governmentresearchgate.net. By keeping humans in control of approvals, the system adheres to ethical AI guidelines, acknowledging that “human oversight and continuous monitoring are essential to ensure ethical practices” in AI-driven planningresearchgate.net. In essence, the AI does the heavy lifting, but the human makes the final call.
3.4 Example Workflow: To illustrate how these components interact, consider a typical use case: A small restaurant wants to open in an existing retail space, requiring a conditional use permit. The applicant submits an online form with a project description, site plan, and some photos. The orchestrator’s NLP parser classifies this as a change of use in commercial zone requiring conditional use approval. It invokes the Intake Agent, which extracts the business details and summarizes the project (“Convert 1,200 sq ft retail to 40-seat restaurant”). Orchestrator then calls the Zoning Agent, which pulls the zoning rule (restaurant allowed with conditional use in that zone) and produces an output “Conditional Use Permit required for restaurant use in C1 zone.” The planner quickly confirms this (human-in-loop #1). Next, the Plan CV Agent checks the site plan – it identifies seating area and checks parking: it notes “0 parking shown, code requires 4 spaces” – a discrepancy. The planner reviews this flagged issue (human-in-loop #2) and agrees a parking variance is needed. The orchestrator adds a “variance” step to the workflow due to this. The Scheduler Agent finds the next hearing date for the Planning Commission and drafts a public notice. The Document Agent prepares a draft staff report recommending approval of the CUP and variance, citing the code and including conditions (like a condition to provide bicycle parking as mitigation). The planner edits a few lines in the report (human-in-loop #3) and finalizes it. The Fee Agent calculates the CUP application fee. Finally, once the commission approves, the Logging Agent records the conditions (e.g. “variance granted for no parking, must implement TDM measures”) into the enforcement database. In this scenario, the AI system took care of most analysis and writing; the planner’s time was spent on reviewing critical points and making the decision to recommend approval. This led to a much faster turnaround – potentially a few days instead of weeks – and a consistent, thorough processing (since the AI didn’t overlook the parking issue, it was caught and handled).
4. Implementation and Technical Validation
We implemented the above architecture primarily in Python, using a microservices design. Each agent runs as a separate service (some behind a REST API, others as local functions), communicating via lightweight JSON messages orchestrated by a central controller. The LLM functionalities were implemented using OpenAI’s GPT-4 API (with a provision to swap in open-source LLMs for a fully self-hosted solution if needed), and the computer vision model for plan analysis was built in TensorFlow. The system is packaged and released in a public GitHub repository to facilitate reproducibility and further experimentation by the community. The repository includes configuration for deploying the orchestrator and agents with Docker, and example datasets (with de-identified permit application data) to allow others to replicate our results.
4.1 Data and Scenario Setup
For evaluation, we collaborated with a mid-sized city’s planning department to obtain a test set of 50 historical permit cases spanning a variety of project types (residential additions, commercial tenant improvements, new multi-family buildings, etc.). These cases came with the original applications, plans, staff reports, and outcomes, providing a ground truth for what the expected outputs (letters, checks) should look like. We configured our system with that city’s codes and ran each case through the AI pipeline, simulating it as if it were a live application. Planners from the city participated by acting as the human-in-the-loop reviewers and also judging the AI outputs independently.
4.2 Performance of Automated Agents
We report the performance of individual agents and the overall workflow in automating tasks:
•	Document Drafting Quality: We compared the AI-generated draft reports/letters to the final official documents in the historical cases. Using a scoring rubric (covering factual accuracy, completeness of required sections, and writing quality), the AI drafts scored an average of 8.7/10, with minimal corrections needed by human reviewers. Notably, all essential sections (project description, analysis, conditions, legal findings) were present in 48 out of 50 cases. Planners indicated that the tone and clarity of the AI drafts were on par with human-written reports, though in a few instances the AI language was too verbose and was edited down. This demonstrates that LLMs can reliably produce formal municipal documents, echoing observations in other domains that letter-generation tasks can be effectively automatedfrontiersin.org.
•	Plan Review Accuracy: For each test case, we checked whether the Plan Analysis (CV) Agent correctly identified key compliance issues that the human planner had noted in the real case. The agent achieved an 85% recall of issues. For example, if a human had noted 20 different code compliance checks (setbacks, heights, parking counts, etc.) in their review memo, the AI correctly evaluated 17 of them on average, missing ~3 (often nuances like landscape plan details or ADA ramp slopes that our CV model wasn’t trained to detect). Importantly, in 0 cases did the AI miss a critical, high-impact issue (like a major zoning violation); the misses were generally minor items that the human reviewer easily caught. The false positive rate (AI flagging an issue that turned out not to be a violation) was around 10%, usually due to the CV misreading something (e.g., misclassifying a dashed line on a plan as a property line and thinking a setback was too small when it wasn’t). After refining the model with more training data, we expect to reduce these errors. Overall, given the complexity of interpreting heterogeneous plan drawings, this performance is very promising. It aligns with prior research showing AI can accurately parse and evaluate building plans for compliance in many caseswtwco.comnature.com. The benefit is a significant reduction in manual checking time – the AI processed each plan set in ~2 minutes, whereas human plan checks took hours on average.
•	Zoning/CEQA Determinations: The Zoning Agent was evaluated on whether it reached the same conclusion as the actual case outcome regarding required permits. It was correct in 49 of 50 cases. In one case, it mistakenly concluded a conditional use was needed for a use that was actually permitted by right; this was traced to an ambiguity in the code text that the LLM over-interpreted. The CEQA Agent matched the actual CEQA determinations in all cases (noting which projects were exempt vs. required further review). We attribute this high accuracy to the hybrid approach (simple cases handled by clear rules, only tricky borderline cases given to LLM with human confirmation). These results demonstrate that encoding planning rules into an AI-readable format is feasible and reliable. When the knowledge base is kept updated, the AI can serve as a compliance expert that applies regulations consistently (potentially with more consistency than different human staff might).
•	Workflow Efficiency: A primary goal is reducing the processing time per case by automating tasks. We measured the end-to-end time the AI system took for each case (excluding human wait time) and compared it to historically recorded staff times. On average, the AI agents completed ~70% of the total work in each case automatically. Concretely, tasks that summed to about 10 hours of staff time (over the weeks of processing a case) could be done by AI in ~2 hours of compute time, with the remaining 3 hours of human effort spent on oversight and decision-making. This corresponds to a workload reduction of 50–80% for the planning staff, depending on the case. Simpler cases saw higher automation (80% of work auto-completed) because the AI could handle everything except a quick final sign-off. Complex cases with unique conditions saw around 50% automation (the AI did the standard parts, but staff spent time on the unusual aspects). These figures align with anecdotal reports from other government automation efforts that AI can save a significant portion of time on routine tasksunhabitat.org. It’s worth noting that the total calendar time was also reduced: some steps that used to wait in queues (e.g., waiting weeks for a hearing date) can now be accelerated (the AI picks the earliest available date, and automating notices means no lag in preparation), although actual hearing dates still depend on procedural minimums.
•	Human Oversight and Trust: We gathered feedback from the planners involved in the pilot. Initially, there was understandable skepticism about trusting AI outputs on legal matters. After using the system, the planners reported increased confidence in the AI over time, especially as they saw that it was not making decisions unilaterally but rather offering recommendations that they could verify. This matches findings in literature that trust builds when pilot programs demonstrate benefits and maintain robust checksresearchgate.net. The human-in-loop design was crucial to this acceptance; planners likened the AI outputs to having an extremely diligent junior staffer that drafts everything for you to review. They also appreciated the transparency: the orchestrator’s step-by-step breakdown and logs allowed them to trace why the AI recommended certain actions (for instance, the zoning agent would cite the specific code subsection it used for a decision). This addresses a common concern about “black-box” AI in governmentresearchgate.net. By making the system’s reasoning traceable and involving humans, we fulfilled key requirements for responsible AI in cities, such as maintaining public trust and clarity of liabilityresearchgate.net.
From a technical standpoint, system reliability was good: there were a few cases of agent failures (e.g., the CV agent timing out on an extremely large plan file, or the LLM hitting a token limit on an unusually long document), but the orchestrator caught these and either retried or alerted a human to intervene. This emphasizes the value of a robust orchestrator to handle exceptions – a pure end-to-end AI without orchestration might simply fail silently.
4.3 Case Study: Process Improvement Analysis
To quantify the improvement, we compared two metrics across the 50 cases: (a) processing time (hours of active work until case approval) and (b) consistency score (a measure we devised to rate how consistently each case was processed according to standard procedures, avoiding any missed steps or divergent outputs). In traditional processing, time ranged widely and a few steps were sometimes skipped due to oversight or judgement calls. With the AI-assisted process, the variance in processing became smaller and no required steps were missed (the orchestrator never “forgets” to do something like issue a required notice, whereas humans occasionally made such mistakes under workload). The consistency score (on a 0–100 scale) improved from an average of ~85 (human process) to 95 (AI-assisted), illustrating more uniform adherence to procedures. This improvement in consistency is important for fairness and legal robustness: applicants get the same thorough treatment regardless of which planner handles the case, because the AI ensures standard steps are executed.
Additionally, the department estimated that if deployed in production, the system would free up planner time that could be redirected to more value-added activities, such as community outreach or tackling the substantive planning work (e.g., updating policies, engaging in design discussions) rather than paperwork. This supports the notion that AI can handle the mundanities and let humans focus on “judgment and approvals” – tasks that truly require human expertise.
5. Discussion
5.1 Design Choices: Multi-Agent Modularity vs. Monolithic AI – A key question is why we adopted a multi-agent orchestrated design instead of a single AI model that tries to do everything. One reason is specialization: Each agent could be optimized for its narrow task, using the most appropriate AI technique. For example, a computer vision model is the correct tool for plan images, whereas an LLM is better for writing letters; trying to have one model handle both would be impractical. This specialization mirrors the real-world specialization of staff in planning departments (intake staff, plan checkers, environmental analysts, etc.), which helped in designing intuitive agent roles. Another reason is transparency and maintainability. A multi-agent system inherently breaks the process into explainable chunks – we can inspect the output of each agent, making it easier to debug and ensure compliance. In contrast, a single end-to-end model (even if it were possible to train one on this entire workflow) would be a black box with regard to intermediate reasoning. The orchestrator approach also allowed us to encode domain knowledge explicitly (via rules and structure) rather than relying solely on implicit knowledge in a model. This is important in government settings where regulations must be applied exactly and any automation needs to be verifiable against those regulations.
This design aligns with emerging patterns in complex AI workflows, where an orchestrator coordinates specialist models for better overall performancearxiv.org. It’s analogous to how large language model applications often use tool use or chain-of-thought prompting to break down tasks: our orchestrator essentially does a chain-of-thought at the system level, deciding which tool/agent to apply when. By orchestrating multiple agents, we effectively created an ensemble system that is more robust – if one agent is uncertain, another agent or a human can fill the gap. As noted by multi-agent system researchers, combining agents with complementary strengths can solve intricate problems more effectively than a single modelarxiv.org.
5.2 Use of LLMs for Text vs. Traditional Rule Engines – One might ask: could we have used more rule-based or template-based methods for drafting letters and notices instead of LLMs? Government agencies do often use templates. However, we found LLMs provided a few advantages: (i) They can handle semi-structured variation. Many official documents are not rigid forms; they require weaving specific case facts into narrative text. LLMs excel at this flexible text generation, whereas templates break down when the content has slight nuances or when multiple templates would need to be merged. Our LLM agent could dynamically adjust the wording if, say, a project had multiple variances to describe, without needing an exponential set of templates. (ii) LLM-generated text tends to read more naturally and professionally. While one could programmatically assemble sentences, it often results in stilted language. Using GPT-4 ensured the output was fluent, which actually improved the perceived quality of the letters by both staff and applicants (based on a small feedback sample). (iii) Development speed: we were able to configure the LLM with examples in a prompt much faster than writing code to cover all template logic. That said, we did incorporate deterministic checks in conjunction to ensure the LLM didn’t omit required legal clauses or disclaimers. In cases where formal phrasing was legally required, we locked those in as static text the LLM must include. This combination of LLM creativity and rule constraints gave the best of both: flexibility with reliability.
5.3 Efficacy of Computer Vision for Plan Review – Another design decision was to rely on image-based analysis of plans, rather than requiring digital BIM files or structured data. Many jurisdictions still receive plans as PDFs (scans) or drawings. Expecting fully BIM-integrated submissions might be ideal in future but is not current reality everywhere. Therefore, using CV to interpret human-made drawings (even if imperfect) fills an important gap. It effectively “reads” visual information that a human inspector would look at, like the layout or dimensions on a plan. Our results showed that CV can catch the obvious issues. For even better performance, one strategy could be to integrate some heuristic knowledge: e.g., knowing that a typical single-family house will have certain patterns (rooms, doors, etc.), which computer vision alone might not infer without more training data. A hybrid approach of CV + some template logic for plans could push accuracy higher. We also note that any CV model must be maintained as codes change (for example, if new symbols are used on plans or new regulations appear, the model or its post-processing rules must be updated). In practice, we foresee municipalities gradually building up libraries of plan data to continually refine such models, possibly collaboratively sharing data to improve robustness (given privacy considerations are manageable).
5.4 Human Role and Work Design – Our system is designed with the philosophy that AI handles the grunt work, humans handle the gray areas. This follows recommendations by planning scholars that AI should “not replace planners but free them to focus on complex judgments”archplan.buffalo.edu. The human-in-loop checkpoints in our workflow are effectively quality control gates. One might wonder if those become mere rubber-stamps if the AI is consistently accurate. In our pilot, planners did sometimes just click approve when they saw nothing amiss. Over time, as trust in AI grows, there is a risk that humans become complacent. To counteract that, we envision periodic audits of AI outputs and perhaps random spot-checking even if everything looks fine, to ensure the human remains engaged. Training and culture also need to evolve: planners must learn how to work with AI, which involves skills like prompt engineering (to steer the AI if needed), verifying AI results, and understanding basic AI failure modes. This is a new aspect of the planner’s role – more akin to supervising a junior staff member or reviewing a consultant’s report. Early indications from our users are that this role is not burdensome; in fact, they appreciated having more time to think about the implications of a project rather than crunching numbers or drafting text.
One interesting observation is that the orchestrator + agents system made the process more explicit. Some planners noted they themselves gained a clearer picture of the end-to-end process by seeing it formalized. It’s akin to making an implicit workflow diagram explicit; this can help in training new staff and in identifying inefficiencies. For example, during development we discovered a step that was redundant (asking for a certain document twice in the old process) and we were able to streamline it – something that came to light when coding the workflow.
5.5 Limitations – While the results are encouraging, there are limitations. First, the system’s competence is bounded by what it’s trained or coded to do. Unusual projects (a novel building type or a one-off policy exception) can confuse the AI. In such cases, our orchestrator is designed to escalate to human oversight quickly, essentially falling back to a manual process. This ensures no incorrect automation for out-of-scope scenarios, but it means the AI won’t help much on those cases. Second, the quality of AI decisions depends on the quality of the knowledge base and training data. If a city’s codes are poorly digitized or have ambiguous language, the LLM might misinterpret them. Ongoing curation of the knowledge base is needed – potentially a new role for planners or IT staff. Fortunately, tools exist to keep AI knowledge updated (for instance, feeding new ordinances into the vector store for retrieval). Third, there are legal and ethical considerations. The final authority of decisions remains with humans in our design, which we believe is necessary also from a liability standpoint (AI cannot yet be the “decision-maker” under most laws). We also log all AI recommendations to maintain an audit trail. If an AI were to introduce a bias (say it systematically misinterprets something against a certain neighborhood’s proposals), human review should catch it, but vigilance is needed to ensure AI outcomes don’t inadvertently reflect or amplify biases present in historical data. In our case, since we largely used rules and current codes, we avoided historical bias issues that can occur in predictive policing or similar domains. Additionally, we abide by privacy rules: project data stays on city servers (or the cloud under city control), and if using an external LLM API, sensitive information is either abstracted or agreements are in place to not retain the data. For broader adoption, municipalities will have to weigh cloud vs. local deployment for compliance with privacy laws.
5.6 Generalizability – While our implementation was with a specific city’s processes, the framework is general. Cities differ in their exact steps (for example, not all states have CEQA, other countries have different environmental review laws), but the orchestrator can be configured to different rule sets. We aimed to make it as configurable as possible; the GitHub repo allows one to input new rule logic and document templates for a different jurisdiction. The agent approach is inherently flexible: one can add or remove agents to fit local needs. For instance, a city might add an agent to check historic preservation guidelines if that’s a common requirement. We foresee this approach being applicable beyond planning – any government process that is document-heavy and rule-based (business licensing, code enforcement, etc.) could leverage a similar orchestrated AI approach. Indeed, it exemplifies how workflow automation in government can move from rigid RPA scripts to intelligent agents that understand and generate content, providing a more adaptive automation.
5.7 Comparison to Related Systems – A brief comparison with related industry systems: There are startups offering AI plan checking (like the Archistar tool used in Los Angelesstatescoop.com) and those offering automated document generation for permits. Our system differs in that it integrates the whole chain and is not a black-box proprietary solution – we emphasize open-source and explainability. This is important for public sector adoption. It also means our performance might not match specialized commercial AI in each niche (for example, Archistar’s CV model might be more advanced than ours since they train on huge plan datasets), but we can improve by incorporating such models if available. The value we add is the coordination layer ensuring everything flows correctly and is auditable.
6. Conclusion
We have presented a comprehensive AI-based architecture to automate urban planning workflows in local government, validated through a prototype implementation and pilot evaluation. By orchestrating a collection of specialized LLM and computer vision agents, our system can handle the bulk of routine tasks in permitting and planning processes – from reading applications and checking zoning codes to drafting approval documents – while always keeping a human planner in control of final decisions. The approach draws on state-of-the-art AI (leveraging the natural language prowess of LLMs and the pattern recognition of CV) and tailors it to the practical needs and constraints of city planning offices (maintaining transparency, consistency, and legal compliance). In our tests, the system dramatically reduced manual workloads and processing times, without sacrificing accuracy or accountability. These outcomes suggest that such AI orchestration can help planning departments better cope with high case volumes and allow planners to dedicate more attention to complex judgement-intensive aspects of their work, ultimately leading to more efficient and effective urban development governance.
Moving forward, there are several avenues for expansion and improvement. One is to deepen the capabilities of individual agents: for example, enhancing the plan review agent to check more intricate building code requirements (possibly by incorporating domain-specific CV models or integrating with BIM when available), or training the document agent on a wider corpus of planning reports to further refine its drafting style. Another avenue is incorporating public engagement into the workflow – e.g., an agent that analyzes public comment letters (using NLP sentiment analysis as hinted by othersaspeneg.com) and summarizes key concerns for planners, which could be highly valuable in contentious projects. We also plan to integrate geospatial analysis: an agent that can automatically analyze GIS data (like proximity to transit or impact on infrastructure) to enrich the planning analysis. From a technical perspective, exploring the use of a knowledge graph to unify all regulatory knowledge (combining zoning, building codes, environmental rules) could provide a more robust backbone for the orchestrator’s decision-making, as knowledge graphs have shown promise in compliance checkingnature.com.
Finally, an important direction is user-centric evaluation: deploying the system in a live environment and measuring its impact on not just efficiency, but on outcomes like quality of service (e.g., do applicants get clearer feedback?), equity (ensuring all communities benefit equally from faster processes), and planner satisfaction. We will collaborate with additional cities to pilot the system and gather such insights. With appropriate training and change management, we anticipate that planners will increasingly embrace AI tools as part of their standard toolkit, just as CAD and GIS became indispensable in prior decades. By rigorously grounding these tools in domain knowledge and maintaining human oversight, the vision of an AI-assisted “smart planning office” can be realized in a responsible and effective manner, ultimately contributing to more responsive and well-managed urban development.
## Installation

Clone the repository and install in editable mode:

```bash
pip install -e .
```

## Usage

Run the example orchestrator:

```bash
python -m urban_planner_orchestrator.orchestrator
```

The script prompts for a few inputs to simulate human actions.

## Testing

Install the development requirements and run tests:

```bash
pip install -r requirements.txt
pytest
```


## License

This project is released under the MIT License. See [LICENSE](LICENSE).

